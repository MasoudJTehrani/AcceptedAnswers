{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbcbWbgtyds4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4b00cc-91b6-4ef3-d027-617d54e98859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Datasets"
      ],
      "metadata": {
        "id": "KX2xs9LN9jkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_csv(\"/content/drive/MyDrive/Colab Files/TM-Project/Phase3/PPnewQuestionsPH3.csv\")\n",
        "comments = pd.read_csv(\"/content/drive/MyDrive/Colab Files/TM-Project/Phase3/PPnewCommentsPH3.csv\")\n",
        "answers = pd.read_csv(\"/content/drive/MyDrive/Colab Files/TM-Project/Phase3/PPnewAnswersPH3.csv\")"
      ],
      "metadata": {
        "id": "mt-WcrPK7z13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Files/TM-Project/Phase3/positive-words.txt', errors='ignore') as f:\n",
        "    positives1 = f.readlines()\n",
        "with open('/content/drive/MyDrive/Colab Files/TM-Project/Phase3/negative-words.txt', errors='ignore') as f:\n",
        "    negatives1 = f.readlines()\n",
        "negatives = []\n",
        "positives = []\n",
        "for i in negatives1:\n",
        "  negatives.append(i[:-1])\n",
        "for i in positives1:\n",
        "  positives.append(i[:-1])"
      ],
      "metadata": {
        "id": "CSIzPyrR73BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PreProcessing Datasets"
      ],
      "metadata": {
        "id": "rGlr3u769mZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = set(nltk.corpus.words.words())\n",
        "stop_words = set(stopwords.words('english'))\n",
        "negativeMakers = [\"don't\", \"dont\", \"isn't\", \"isnt\", \"wouldn't\", \"wouldnt\",  \"couldn't\",  \"couldnt\",  \"weren't\",  \"werent\"\n",
        ",\"doesn't\", \"doesnt\", \"weren't\", \"werent\", \"aren't\", \"arent\", \"didn't\",\" didnt\", \"aren't\", \"arent\", \"shan't\", \"shant\"\n",
        ",\"hadn't\", \"hadnt\", \"won't\", \"wont\", \"mustn't\",\"mustnt\" ,\"not\" ,\"needn't\", \"neednt\", \"mightn't\", \"mightnt\", \"no\"\n",
        ",\"wasn't\", \"wasnt\", \"nor\", \"neither\"]\n",
        "thanks = ['thanks', 'thank', 'tanks', 'tnx']\n",
        "dontRemove = [i for i in negativeMakers]\n",
        "for j in thanks:\n",
        "  dontRemove.append(j)\n",
        "dontRemove.append(\"but\")\n",
        "stopWords = [ w for w in stop_words if w not in dontRemove]"
      ],
      "metadata": {
        "id": "MlTlhJi50lu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comments"
      ],
      "metadata": {
        "id": "Dr9nu8bE9wV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "commentsWordsList =[]\n",
        "ppComments = []\n",
        "for c in comments['PPText'].to_numpy():\n",
        "  if(type(c) != float):\n",
        "    pprow = []\n",
        "    for i in c.split():\n",
        "      if len(i) > 0 and i not in stopWords:\n",
        "        if i in words or i in negativeMakers or i in thanks:\n",
        "          pprow.append(i)\n",
        "          commentsWordsList.append(i)\n",
        "    ppComments.append(pprow)\n",
        "  else:\n",
        "    ppComments.append([])\n",
        "\n",
        "commentsWordsCount = len(commentsWordsList)"
      ],
      "metadata": {
        "id": "-kNutCr59p__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answers"
      ],
      "metadata": {
        "id": "H7cwz4hx-9Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answersWordsList =[]\n",
        "ppAnswers = []\n",
        "for a in answers['PPText'].to_numpy():\n",
        "  if(type(a) != float):\n",
        "    pprow = []\n",
        "    for i in a.split():\n",
        "      if len(i) > 0 and i not in stopWords:\n",
        "        if i in words or i in negativeMakers or i in thanks:\n",
        "          pprow.append(i)\n",
        "          answersWordsList.append(i)\n",
        "    ppAnswers.append(pprow)\n",
        "  else:\n",
        "    ppAnswers.append([])\n",
        "\n",
        "answersWordsCount = len(answersWordsList)"
      ],
      "metadata": {
        "id": "2dHn30PR_AMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions"
      ],
      "metadata": {
        "id": "bYuTFxQA_QoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questionsWordsList =[]\n",
        "ppQuestions = []\n",
        "for q in questions['PPText'].to_numpy():\n",
        "  if(type(q) != float):\n",
        "    pprow = []\n",
        "    for i in q.split():\n",
        "      if len(i) > 0 and i not in stopWords:\n",
        "        if i in words or i in negativeMakers or i in thanks:\n",
        "          pprow.append(i)\n",
        "          questionsWordsList.append(i)\n",
        "    ppQuestions.append(pprow)\n",
        "  else:\n",
        "    ppQuestions.append([])\n",
        "\n",
        "questionsWordsCount = len(questionsWordsList)"
      ],
      "metadata": {
        "id": "0OpK3f9j_QDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "rCec144_LChE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeDiscrete(inp):\n",
        "  inp = pd.DataFrame({\"inp\":inp})\n",
        "  trans = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans') #{‘uniform’, ‘quantile’, ‘kmeans’}\n",
        "  data_trans = trans.fit_transform(inp)\n",
        "  result = []\n",
        "  for d in data_trans:\n",
        "    result.append(d[0])\n",
        "  return result"
      ],
      "metadata": {
        "id": "JFYzzSPA73cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction"
      ],
      "metadata": {
        "id": "K9rYuHupCu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Number of answers"
      ],
      "metadata": {
        "id": "CQj7xj7sCxCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answersQuestionID = answers['ParentId(QuestionId)'].to_numpy()\n",
        "unique, counts = np.unique(answersQuestionID, return_counts=True)\n",
        "countsdict = dict(zip(unique, counts))\n",
        "nAnswers = []\n",
        "for a in answersQuestionID:\n",
        "  nAnswers.append(countsdict[a])"
      ],
      "metadata": {
        "id": "hEeDdeJbC4QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words count"
      ],
      "metadata": {
        "id": "TpQc-aN1Ek4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answers"
      ],
      "metadata": {
        "id": "421mzBoHE41x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wCountAnswers = []\n",
        "for a in ppAnswers:\n",
        "  wCountAnswers.append(len(a))"
      ],
      "metadata": {
        "id": "hk56ckSJEuiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wCountA = makeDiscrete(wCountAnswers)\n",
        "wCountA = wCountAnswers"
      ],
      "metadata": {
        "id": "AZJwiOCLqFvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Questions"
      ],
      "metadata": {
        "id": "JSthzP10FKUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wCountQuestions = []\n",
        "for a in range(len(ppAnswers)):\n",
        "  wCountQuestions.append(len(ppQuestions[questions.loc[ questions['Id'] == answers.iloc[a,1] ].index.tolist()[0]]))"
      ],
      "metadata": {
        "id": "01FGq_oDFLsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wCountQ = makeDiscrete(wCountQuestions)\n",
        "wCountQ = wCountQuestions"
      ],
      "metadata": {
        "id": "rP9Mw-tms8vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comments"
      ],
      "metadata": {
        "id": "VZ8IDJfpHZSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wCountComments = []\n",
        "for a in answers['Id'].to_numpy():\n",
        "  commentsIndex = comments.loc[ comments['PostId(AnswerId)'] == a].index.tolist()\n",
        "  count = 0\n",
        "  for i in commentsIndex:\n",
        "    count += len(ppComments[i])\n",
        "  wCountComments.append(count)"
      ],
      "metadata": {
        "id": "U-XivbeWHex4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wCountC = makeDiscrete(wCountComments)\n",
        "wCountC = wCountComments"
      ],
      "metadata": {
        "id": "LQ86CNZPtFoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comments count"
      ],
      "metadata": {
        "id": "bUKvv7HzuJY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nComments = []\n",
        "for a in answers['Id'].to_numpy():\n",
        "  nComments.append(len(comments.loc[ comments['PostId(AnswerId)'] == a]))"
      ],
      "metadata": {
        "id": "UQ8zNDTouXO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment"
      ],
      "metadata": {
        "id": "9CEAwVz5niwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiments extracted by Sentid4sd \"https://github.com/collab-uniba/Senti4SD\"\n",
        "path = \"/content/drive/MyDrive/Colab Files/TM-Project/Phase3/Predict(Answer)\"\n",
        "sentiment = []\n",
        "for file in os.listdir(path):\n",
        "  file_path = f\"{path}/{file}\"\n",
        "  csv = pd.read_csv(file_path)\n",
        "  for i in range(len(csv)):\n",
        "    sent = csv.loc[csv['Row'] == 't'+str(i), 'Predicted'].iloc[0]\n",
        "    if sent == \"positive\":\n",
        "      sentiment.append(0)\n",
        "    elif sent == \"negative\":\n",
        "      sentiment.append(1)\n",
        "    else:\n",
        "      sentiment.append(2)"
      ],
      "metadata": {
        "id": "JHAuDoRanotp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Thanks Included"
      ],
      "metadata": {
        "id": "E06ZMi1Nu6L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thanksIncluded = np.zeros(len(ppAnswers))\n",
        "answersIdList = answers['Id'].to_numpy()\n",
        "for a in range(len(ppAnswers)):\n",
        "  commentsIndex = comments.loc[ comments['PostId(AnswerId)'] == answersIdList[a]].index.tolist()\n",
        "  flag = 0\n",
        "  for i in commentsIndex:\n",
        "    theComment = ppComments[i]\n",
        "    for t in thanks:\n",
        "      if t in theComment:\n",
        "        thanksIncluded[a] = 1\n",
        "        flag = 1\n",
        "        break\n",
        "    if flag == 1:\n",
        "      break"
      ],
      "metadata": {
        "id": "QlfTOZdiu_Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##But Included"
      ],
      "metadata": {
        "id": "usVQu3Xdu857"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "butIncluded = np.zeros(len(ppAnswers))\n",
        "answersIdList = answers['Id'].to_numpy()\n",
        "for a in range(len(ppAnswers)):\n",
        "  commentsIndex = comments.loc[ comments['PostId(AnswerId)'] == answersIdList[a]].index.tolist()\n",
        "  for i in commentsIndex:\n",
        "    theComment = ppComments[i]\n",
        "    if 'but' in theComment:\n",
        "      butIncluded[a] = 1\n",
        "      break"
      ],
      "metadata": {
        "id": "xB_kPWAFwVfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Negative maker words count"
      ],
      "metadata": {
        "id": "Opyt9JxaxfIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nNegative = []\n",
        "answersIdList = answers['Id'].to_numpy()\n",
        "for a in range(len(ppAnswers)):\n",
        "  commentsIndex = comments.loc[ comments['PostId(AnswerId)'] == answersIdList[a]].index.tolist()\n",
        "  count = 0\n",
        "  for i in commentsIndex:\n",
        "    theComment = ppComments[i]\n",
        "    for n in negativeMakers:\n",
        "      if n in theComment:\n",
        "        count += 1\n",
        "  if wCountComments[a] != 0:\n",
        "    nNegative.append(count / wCountComments[a])\n",
        "  else:\n",
        "    nNegative.append(0)"
      ],
      "metadata": {
        "id": "TPQ0mKdvxi4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nMakers = makeDiscrete(nNegative)\n",
        "nMakers = nNegative"
      ],
      "metadata": {
        "id": "YZYhDvD0xQAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##P(y)/Words count"
      ],
      "metadata": {
        "id": "wrj4ZVPAygDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comments"
      ],
      "metadata": {
        "id": "1CDNb3XpzqlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyPositive = []\n",
        "pyNegative = []\n",
        "existingPositives = []\n",
        "existingNegatives = []\n",
        "for y in positives:\n",
        "  count = commentsWordsList.count(y)\n",
        "  if count != 0:\n",
        "    pyPositive.append( round((count / commentsWordsCount) * 1000000 , 2) )\n",
        "    existingPositives.append(y)\n",
        "for y in negatives:\n",
        "  count = commentsWordsList.count(y)\n",
        "  if count != 0:\n",
        "    pyNegative.append( round((count / commentsWordsCount) * 1000000 , 2) )\n",
        "    existingNegatives.append(y)\n"
      ],
      "metadata": {
        "id": "0vzXGkK3y4jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyComments = []\n",
        "answersIdList = answers['Id'].to_numpy()\n",
        "for a in range(len(answersIdList)):\n",
        "  commentsIndex = comments.loc[ comments['PostId(AnswerId)'] == answersIdList[a]].index.tolist()\n",
        "  count = 0\n",
        "  for i in commentsIndex:\n",
        "    theComment = ppComments[i]\n",
        "    if len(theComment) != 0:\n",
        "      for pos in range(len(existingPositives)):\n",
        "        if existingPositives[pos] in theComment:\n",
        "          count += pyPositive[pos]\n",
        "      for neg in range(len(existingNegatives)):\n",
        "        if existingNegatives[neg] in theComment:\n",
        "          count -= pyNegative[neg]\n",
        "  if count != 0:\n",
        "    pyComments.append(count/ wCountComments[a])\n",
        "  else:\n",
        "    pyComments.append(0)"
      ],
      "metadata": {
        "id": "oxfAI0yN4e24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If i want to remove this, I have to add Round in pyComments.append(count/ wCountComments[a])\n",
        "#pyC = makeDiscrete(pyComments)\n",
        "pyC = pyComments"
      ],
      "metadata": {
        "id": "v_p-DEUC6WoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answers"
      ],
      "metadata": {
        "id": "PfHKF-Vz7EEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyPositive = []\n",
        "pyNegative = []\n",
        "existingPositives = []\n",
        "existingNegatives = []\n",
        "for y in positives:\n",
        "  count = answersWordsList.count(y)\n",
        "  if count != 0:\n",
        "    pyPositive.append( round((count / answersWordsCount) * 1000000 , 2) )\n",
        "    existingPositives.append(y)\n",
        "for y in negatives:\n",
        "  count = answersWordsList.count(y)\n",
        "  if count != 0:\n",
        "    pyNegative.append( round((count / answersWordsCount) * 1000000 , 2) )\n",
        "    existingNegatives.append(y)"
      ],
      "metadata": {
        "id": "esiJHDHw7Hwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyAnswers = []\n",
        "for a in range(len(ppAnswers)):\n",
        "  count = 0\n",
        "  theAnswer = ppAnswers[a]\n",
        "  for pos in range(len(existingPositives)):\n",
        "    if existingPositives[pos] in theAnswer:\n",
        "      count += pyPositive[pos]\n",
        "  for neg in range(len(existingNegatives)):\n",
        "    if existingNegatives[neg] in theAnswer:\n",
        "      count -= pyNegative[pos]\n",
        "  if count != 0:\n",
        "    pyAnswers.append(count/ wCountAnswers[a])\n",
        "  else:\n",
        "    pyAnswers.append(0)"
      ],
      "metadata": {
        "id": "L8rZKyXk7Tl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If i want to remove this, I have to add Round in pyComments.append(count/ wCountComments[a])\n",
        "#pyA = makeDiscrete(pyAnswers)\n",
        "pyA = pyAnswers"
      ],
      "metadata": {
        "id": "SO5PGPL87zMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordnet Similarity"
      ],
      "metadata": {
        "id": "oO5VFTad97Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Questions and Answers"
      ],
      "metadata": {
        "id": "HyrURmyV9_fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allSynsets = {}\n",
        "for a in np.unique(np.concatenate((questionsWordsList, answersWordsList, commentsWordsList))):\n",
        "  syns = wn.synsets(a)\n",
        "  if len(syns) == 0:\n",
        "    allSynsets[a] = 0\n",
        "  else:\n",
        "    allSynsets[a] = syns[0]"
      ],
      "metadata": {
        "id": "f_fb67zsTNEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simDict = {}\n",
        "answersParentIdList = answers['ParentId(QuestionId)'].to_numpy()\n",
        "questionAnswerSim = []\n",
        "for a in range(len(answersParentIdList)):\n",
        "  theQuestion = ppQuestions[questions.loc[ questions['Id'] == answersParentIdList[a]].index.tolist()[0]]\n",
        "  theAnswer = ppAnswers[a]\n",
        "  wordsCount = len(theQuestion) + len(theAnswer)\n",
        "  wup_sim = 0\n",
        "  for qw in theQuestion:\n",
        "    for aw in theAnswer:\n",
        "      if simDict.get(qw+aw) == None:\n",
        "        if simDict.get(aw+qw) == None:\n",
        "          first = allSynsets[qw]\n",
        "          if type(first) != int:\n",
        "            second = allSynsets[aw]\n",
        "            if type(second) != int:\n",
        "              sim = first.wup_similarity(second)\n",
        "              if sim != None:\n",
        "                simDict[qw+aw] = sim\n",
        "                simDict[aw+qw] = sim\n",
        "                wup_sim += sim\n",
        "              else:\n",
        "                simDict[qw+aw] = 0\n",
        "                simDict[aw+qw] = 0\n",
        "        else:\n",
        "          wup_sim += simDict[aw+qw]\n",
        "      else:\n",
        "        wup_sim += simDict[qw+aw]\n",
        "  questionAnswerSim.append(wup_sim/wordsCount)"
      ],
      "metadata": {
        "id": "msSKKuYeR_ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If i want to remove this, I have to add Round\n",
        "#QAsim = makeDiscrete(questionAnswerSim)\n",
        "QAsim = questionAnswerSim"
      ],
      "metadata": {
        "id": "xYmxS1CuF4QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answers and Comments"
      ],
      "metadata": {
        "id": "SsGSiZkZ-BfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answersIdList = answers['Id'].to_numpy()\n",
        "answerCommentSim = []\n",
        "for a in range(len(answersIdList)):\n",
        "  CommentsIndex = comments.loc[ comments['PostId(AnswerId)'] == answersIdList[a]].index.tolist()\n",
        "  theAnswer = ppAnswers[a]\n",
        "  wordsCount = len(theAnswer) + wCountComments[a]\n",
        "  wup_sim = 0\n",
        "  for i in CommentsIndex:\n",
        "    theComment = ppComments[i]\n",
        "    for qw in theComment:\n",
        "      for aw in theAnswer:\n",
        "        if simDict.get(qw+aw) == None:\n",
        "          if simDict.get(aw+qw) == None:\n",
        "            first = allSynsets[qw]\n",
        "            if type(first) != int:\n",
        "              second = allSynsets[aw]\n",
        "              if type(second) != int:\n",
        "                sim = first.wup_similarity(second)\n",
        "                if sim != None:\n",
        "                  simDict[qw+aw] = sim\n",
        "                  simDict[aw+qw] = sim\n",
        "                  wup_sim += sim\n",
        "                else:\n",
        "                  simDict[qw+aw] = 0\n",
        "                  simDict[aw+qw] = 0\n",
        "          else:\n",
        "            wup_sim += simDict[aw+qw]\n",
        "        else:\n",
        "          wup_sim += simDict[qw+aw]\n",
        "  answerCommentSim.append(wup_sim/wordsCount)"
      ],
      "metadata": {
        "id": "xGKxma3D-Hcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If i want to remove this, I have to add Round\n",
        "#ACsim = makeDiscrete(answerCommentSim)\n",
        "ACsim = answerCommentSim"
      ],
      "metadata": {
        "id": "VFaAHEZ0Pxs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Labeling"
      ],
      "metadata": {
        "id": "Ilnx0MY-c_Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acceptedsId = questions['AcceptedAnswerId'].to_numpy()\n",
        "answersId = answers['Id'].to_numpy()\n",
        "label = []\n",
        "for a in answersId:\n",
        "  if a in acceptedsId:\n",
        "    label.append(1)\n",
        "  else:\n",
        "    label.append(0)"
      ],
      "metadata": {
        "id": "lpMea6Zgc9Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine learning"
      ],
      "metadata": {
        "id": "MCY6ixqecHQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = pd.DataFrame({'NoA':nAnswers, 'WCA': wCountA, 'WCQ': wCountQ, 'WCC': wCountC, 'CC': nComments,\n",
        "                        #'tnx': thanksIncluded, \"Senti\": sentiment, 'but': butIncluded, 'NMC': nMakers,\n",
        "                        #'PYC': pyC, 'PYA': pyA, 'SQA': QAsim, 'SAC': ACsim, 'Label': label})\n",
        "#mamuli uniform 69.5 ada\n",
        "#dataset = pd.DataFrame({'NoA':nAnswers, 'CC': nComments,\n",
        "                        #'tnx': thanksIncluded, 'but': butIncluded,\n",
        "                        #'Label': label})\n",
        "#quantile 70% ada and random and dt\n",
        "#dataset = pd.DataFrame({'NoA':nAnswers, 'WCA': wCountA, 'CC': nComments,\n",
        "                        #'tnx': thanksIncluded,'but': butIncluded, 'NMC': nMakers,\n",
        "                        #'Label': label})"
      ],
      "metadata": {
        "id": "fVe-4ZGKjafD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection"
      ],
      "metadata": {
        "id": "gHMbl1BTFffg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame({'NoA':nAnswers, 'WCA': wCountA, 'WCQ': wCountQ, 'WCC': wCountC, 'CC': nComments,\n",
        "                        'tnx': thanksIncluded, \"Senti\": sentiment, 'but': butIncluded, 'NMC': nMakers,\n",
        "                        'PYC': pyC, 'PYA': pyA, 'SQA': QAsim, 'SAC': ACsim, 'Label': label})\n",
        "features = dataset.iloc[:, :-1]\n",
        "labels = dataset.iloc[:, -1]"
      ],
      "metadata": {
        "id": "LRYhSSnSNSbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save dataset to use later\n",
        "dataset.to_csv(\"/content/drive/MyDrive/Colab Files/TM-Project/Phase3/ds.csv\", index=False)"
      ],
      "metadata": {
        "id": "JIoji3JAsoZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = pd.read_csv(\"/content/drive/MyDrive/Colab Files/TM-Project/Phase3/ds.csv\")\n",
        "dataset = ds\n",
        "#dataset = ds[['NoA', 'WCA', 'CC', 'tnx', 'PYC', 'NMC', 'Label']]\n",
        "features = dataset.iloc[:, :-1]\n",
        "labels = dataset.iloc[:, -1]"
      ],
      "metadata": {
        "id": "pvAeW9vAs4P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = pd.DataFrame({'NoA':nAnswers, 'WCA': wCountA, 'CC': nComments,\n",
        "                        #'tnx': thanksIncluded, 'PYC': pyC, 'NMC': nMakers,\n",
        "                        #'Label': label})\n",
        "#features = dataset.iloc[:, :-1]\n",
        "#labels = dataset.iloc[:, -1]"
      ],
      "metadata": {
        "id": "TnMkpGZuL5lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KNN"
      ],
      "metadata": {
        "id": "OE1QZi9Jlk3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(algorithm='auto',\n",
        "                           metric='minkowski',\n",
        "                           metric_params=None,\n",
        "                           n_neighbors=8,\n",
        "                           p=2)\n",
        "#Cross validation\n",
        "predicted = cross_val_predict(knn,features,labels,cv=10 ,n_jobs=-1)\n",
        "print(metrics.classification_report(labels, predicted))"
      ],
      "metadata": {
        "id": "HMuqQmhQfEMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e7e4d9-0a01-44e8-da8d-5a184701484c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.81      0.72     21813\n",
            "           1       0.58      0.37      0.45     15462\n",
            "\n",
            "    accuracy                           0.63     37275\n",
            "   macro avg       0.61      0.59      0.59     37275\n",
            "weighted avg       0.62      0.63      0.61     37275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest"
      ],
      "metadata": {
        "id": "fH4EUFBvlmaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Random_Forest_model = RandomForestClassifier(n_estimators=100,criterion=\"entropy\", random_state=0)\n",
        "#Cross validation\n",
        "predicted = cross_val_predict(Random_Forest_model,features,labels,cv=10 ,n_jobs=-1)\n",
        "print(metrics.classification_report(labels, predicted))"
      ],
      "metadata": {
        "id": "_975rbSTiT6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4cbbaa-0706-4610-ac4a-e59a09094ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.78      0.75     21813\n",
            "           1       0.65      0.57      0.61     15462\n",
            "\n",
            "    accuracy                           0.69     37275\n",
            "   macro avg       0.68      0.68      0.68     37275\n",
            "weighted avg       0.69      0.69      0.69     37275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ada Boost"
      ],
      "metadata": {
        "id": "2MqEWWuClor1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=100, random_state = 0)\n",
        "#Cross validation\n",
        "predicted = cross_val_predict(AdaBoost,features,labels,cv=10 ,n_jobs=-1)\n",
        "print(metrics.classification_report(labels, predicted))"
      ],
      "metadata": {
        "id": "7CJx3YFnjKb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d19d09-81cd-4b0f-f6aa-ad1c71dea3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.82      0.77     21813\n",
            "           1       0.68      0.55      0.61     15462\n",
            "\n",
            "    accuracy                           0.71     37275\n",
            "   macro avg       0.70      0.68      0.69     37275\n",
            "weighted avg       0.70      0.71      0.70     37275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree"
      ],
      "metadata": {
        "id": "mCQxPkfBlrqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "predicted = cross_val_predict(DT,features,labels,cv=10 ,n_jobs=-1)\n",
        "print(metrics.classification_report(labels, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1h69Q0ww6ac",
        "outputId": "4dfe7893-b577-4849-faa4-8182323b5052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67     21813\n",
            "           1       0.54      0.55      0.54     15462\n",
            "\n",
            "    accuracy                           0.62     37275\n",
            "   macro avg       0.61      0.61      0.61     37275\n",
            "weighted avg       0.62      0.62      0.62     37275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM"
      ],
      "metadata": {
        "id": "d20sQd2qbMFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel='rbf')\n",
        "predicted = cross_val_predict(svm,features,labels,cv=10 ,n_jobs=-1)\n",
        "print(metrics.classification_report(labels, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGPPOJgWbNRg",
        "outputId": "8519bcef-fc69-4fd1-aa46-ffffc3400c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.79      0.72     21813\n",
            "           1       0.60      0.46      0.52     15462\n",
            "\n",
            "    accuracy                           0.65     37275\n",
            "   macro avg       0.64      0.62      0.62     37275\n",
            "weighted avg       0.64      0.65      0.64     37275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "dtD66kLCkVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Gaussian = GaussianNB()\n",
        "predicted = cross_val_predict(Gaussian,features,labels,cv=10 ,n_jobs=-1)\n",
        "print(metrics.classification_report(labels, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK_WlDyWkUu0",
        "outputId": "47429781-4c5c-4867-85db-a295761c0b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.81      0.74     21813\n",
            "           1       0.64      0.48      0.54     15462\n",
            "\n",
            "    accuracy                           0.67     37275\n",
            "   macro avg       0.66      0.64      0.64     37275\n",
            "weighted avg       0.66      0.67      0.66     37275\n",
            "\n"
          ]
        }
      ]
    }
  ]
}